{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "german-indianapolis",
   "metadata": {},
   "source": [
    "# Derive Snow Depth From ICESat-2 ATL08 and 3DEP at Grand Mesa, CO  \n",
    "Notebook developed by Hannah Besso, adapted from code by Ben Smith and containing code from David Shean for the UW Hackweek 2021\n",
    "\n",
    "### This notebook does the following:\n",
    "* Read in ATL08 data for from openAltimetry using IcyPyx, select a track to analyze, and convert to a geodataframe  \n",
    "* Import 3DEP file, as downloaded from the download_3DEP.ipynb notebook  \n",
    "* Extract 3DEP elevations at ATL08 locations\n",
    "* Account for datum differences between ATL08 and 3DEP elevation products\n",
    "* Calculate and plot snow depth by differencing the snow-on ATL08 elevations from the snow-off 3DEP elevations\n",
    "* Determine mean snow depth along the chosen track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "from rasterio.enums import Resampling\n",
    "import s3fs\n",
    "import requests\n",
    "import icepyx as ipx\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import rasterio as rio\n",
    "import rasterio.plot\n",
    "from rasterio.plot import plotting_extent\n",
    "\n",
    "from scipy.interpolate import RectBivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-danger",
   "metadata": {},
   "source": [
    "## Read ATL08 from Open Altimetry\n",
    "\n",
    "This section contains code written by Ben Smith for reading ATL08 data for Grand Mesa from openAltimetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-algeria",
   "metadata": {},
   "source": [
    "### Import the Sentinel basemap\n",
    "The sentinel basemap is a useful way to visualize map locations on the fly. This notebook does not use the Sentinel basemap very much, but this code is useful to have for future projects, and the lonlims and latlims variables created in this cell are necessary for the CMR query in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL environment variables to efficiently read remote data\n",
    "os.environ['GDAL_DISABLE_READDIR_ON_OPEN']='EMPTY_DIR' \n",
    "os.environ['AWS_NO_SIGN_REQUEST']='YES' \n",
    "\n",
    "# SAR Data are stored in a public S3 Bucket\n",
    "url = 's3://sentinel-s1-rtc-indigo/tiles/RTC/1/IW/12/S/YJ/2016/S1B_20161121_12SYJ_ASC/Gamma0_VV.tif'\n",
    "\n",
    "# These Cloud-Optimized-Geotiff (COG) files have 'overviews', low-resolution copies for quick visualization\n",
    "XR=[725000.0, 767000.0]\n",
    "YR=[4.30e6, 4.34e6]\n",
    "# open the dataset\n",
    "da = rxr.open_rasterio(url, overview_level=1).squeeze('band')#.clip_box([712410.0, 4295090.0, 797010.0, 4344370.0])\n",
    "da=da.where((da.x>XR[0]) & (da.x < XR[1]), drop=True)\n",
    "da=da.where((da.y>YR[0]) & (da.y < YR[1]), drop=True)\n",
    "dx=da.x[1]-da.x[0]\n",
    "SAR_extent=[da.x[0]-dx/2, da.x[-1]+dx/2, np.min(da.y)-dx/2, np.max(da.y)+dx/2]\n",
    "\n",
    "# Prepare coordinate transformations into the basemap coordinate system\n",
    "from pyproj import Transformer, CRS\n",
    "crs=CRS.from_wkt(da['spatial_ref'].spatial_ref.crs_wkt)\n",
    "to_image_crs=Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "to_geo_crs=Transformer.from_crs(crs, crs.geodetic_crs)\n",
    "\n",
    "corners_lon, corners_lat=to_geo_crs.transform(np.array(XR)[[0, 1, 1, 0, 0]], np.array(YR)[[0, 0, 1, 1, 0]])\n",
    "lonlims=[np.min(corners_lat), np.max(corners_lat)]\n",
    "latlims=[np.min(corners_lon), np.max(corners_lon)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-theorem",
   "metadata": {},
   "source": [
    "### Use IcePyx to query CMR (NASA EOSDIS Common Metadata Repository) for available granules\n",
    "\n",
    "(Can probably do this with openAltimetry too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a = ipx.Query('ATL08', [lonlims[0], latlims[0], lonlims[1], latlims[1]], ['2020-01-01','2020-6-30'], \\\n",
    "                          start_time='00:00:00', end_time='23:59:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_re=re.compile('ATL.._(?P<year>\\d\\d\\d\\d)(?P<month>\\d\\d)(?P<day>\\d\\d)\\d+_(?P<track>\\d\\d\\d\\d)')\n",
    "\n",
    "date_track=[]\n",
    "for count, item in enumerate(region_a.granules.avail):\n",
    "    granule_info=ATLAS_re.search(item['producer_granule_id']).groupdict()\n",
    "    date_track += [ ('-'.join([granule_info[key] for key in ['year', 'month', 'day']]), granule_info['track'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-panel",
   "metadata": {},
   "source": [
    "### Define a function to read ATL08 from OpenAltimetry\n",
    "\n",
    "Uses the latlim and lonlim coordinates, along with the date and track information from the above cell to request data from OpenAltimetry. The output (as specified in the function notes) is a dictionary of the data, with individual tracks, dates, and variables as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OA_ATL08(date_track, lonlims, latlims, beamnames=[\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]):\n",
    "    '''\n",
    "    retrieve ICESat2 ATL08 data from OpenAltimetry\n",
    "    \n",
    "    Inputs:\n",
    "        date_track: a list of tuples.  Each contains a date string \"YYYY-MM-DD\" and track number (4-character string)\n",
    "        lonlims: longitude limits for the search\n",
    "        latlims: latitude limits for the search\n",
    "        beamnames: list of strings for the beams\n",
    "    outputs:\n",
    "        a dict containing ATL08 data by beam name\n",
    "    \n",
    "    Due credit:\n",
    "        Much of this code was borrowed Philipp Arndt's Pond Picker repo: https://github.com/fliphilipp/pondpicking\n",
    "    '''\n",
    "      \n",
    "    IS2_data={}\n",
    "    for this_dt in date_track:\n",
    "        this_IS2_data={}\n",
    "        for beamname in beamnames:\n",
    "            oa_url = 'https://openaltimetry.org/data/api/icesat2/atl08?minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={trackid}&beamName={beamname}&outputFormat=json&date={date}&client=jupyter'\n",
    "            oa_url = oa_url.format(minx=lonlims[0],miny=latlims[0],maxx=lonlims[1], maxy=latlims[1], \n",
    "                                   trackid=this_dt[1], beamname=beamname, date=this_dt[0], sampling='true')\n",
    "            #.conf_ph = ['Noise','Buffer', 'Low', 'Medium', 'High']\n",
    "            if True:\n",
    "                r = requests.get(oa_url)\n",
    "                data = r.json()\n",
    "                D={}\n",
    "                D['lat_seg'] = []\n",
    "                D['lon_seg'] = []\n",
    "                D['h_te_best_fit'] = []\n",
    "                D['h_canopy']=[]\n",
    "                for series in data['series']:\n",
    "                    for p in series['lat_lon_elev_canopy']:\n",
    "                        D['lat_seg'].append(p[0])\n",
    "                        D['lon_seg'].append(p[1])\n",
    "                        if p[2] is not None:\n",
    "                            D['h_te_best_fit'].append(p[2])\n",
    "                        else:\n",
    "                            D['h_te_best_fit'].append(np.NaN)\n",
    "                        if p[3] is not None:\n",
    "                            D['h_canopy'].append(p[3])\n",
    "                        else:\n",
    "                            D['h_canopy'].append(np.NaN)\n",
    "                D['x_seg'], D['y_seg']=to_image_crs.transform(D['lat_seg'], D['lon_seg'])\n",
    "                for key in D:\n",
    "                    D[key]=np.array(D[key])\n",
    "                if len(D['lat_seg']) > 0:\n",
    "                    this_IS2_data[beamname]=D\n",
    "            #except Exception as e:\n",
    "            #    print(e)\n",
    "            #    pass\n",
    "        if len(this_IS2_data.keys()) > 0:\n",
    "            IS2_data[this_dt] = this_IS2_data\n",
    "    return IS2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-extra",
   "metadata": {},
   "source": [
    "### Read the ATL08 Data from Open Altimetery\n",
    "And choose a single track to analyze, converting into a geodataframe for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to read the data\n",
    "D08 = get_OA_ATL08(date_track, lonlims, latlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# March 11 is the date closest to the SnowEx field campaign (late January through Feb 14) with ICESat-2 data intersecting Grand Mesa.\n",
    "# Use the date and track keys to pull out the data for March 11:\n",
    "\n",
    "march_11 = D08['2020-03-11','1156']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the ICESat-2 data we chose does in fact intersect Grand Mesa by plotting it with the Sentinel basemap\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "\n",
    "for beam, D in march_11.items():\n",
    "    plt.plot(D['x_seg'], D['y_seg'], '.', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a geodataframe for easier analysis, using the UTM Zone 12N projection for Grand Mesa\n",
    "\n",
    "gdf = gpd.GeoDataFrame(march_11['gt1l'], crs='EPSG:32612', geometry=gpd.points_from_xy(march_11['gt1l']['x_seg'], march_11['gt1l']['y_seg'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a first look at the data\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-terrorist",
   "metadata": {},
   "source": [
    "### Clip the ATL08 data to only the flat top of the Mesa\n",
    "\n",
    "We don't want to calculate snow depth for the steeply sloping areas surrounding the Mesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a polygon of only the flat top of the Mesa\n",
    "\n",
    "poly_fn = \"/home/jovyan/space_lasers/notebooks/GM_tight_poly_32612.geojson\"\n",
    "poly_gm = gpd.read_file(poly_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the ATL08 geodataframe to the flat top of the Mesa:\n",
    "\n",
    "march_11_clip = gpd.clip(gdf, poly_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-wednesday",
   "metadata": {},
   "source": [
    "## Import 3DEP data\n",
    "Use the tiled 3DEP tif created in the download_3DEP.ipynb notebook here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the file path for the tiff created in the download_3DEP.ipynb notebook:\n",
    "\n",
    "tif_fn = '/home/jovyan/space_lasers/notebooks/gm_3dep_1m_lidar_tiles.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the 3DEP tiff and rescale it from 1m resolution to 3m resolution to speed up computation time \n",
    "# (and to make it possible to work with, under the 8GB limit on the Hackweek JupyterHub!)\n",
    "# Also reproject to match the ATL08 data projection of UTM Zone 12N\n",
    "\n",
    "lidar_ds=rxr.open_rasterio(tif_fn)\n",
    "#resample the DTM to ~3m:\n",
    "scale_factor = 1/3\n",
    "new_width = int(lidar_ds.rio.width * scale_factor)\n",
    "new_height = int(lidar_ds.rio.height * scale_factor)\n",
    "\n",
    "#reproject the horizontal CRS to match ICESat-2\n",
    "UTM_wgs84_crs=CRS.from_epsg(32612)\n",
    "lidar_3m = lidar_ds.rio.reproject(\n",
    "    UTM_wgs84_crs,\n",
    "    shape=(new_height, new_width),\n",
    "    resampling=Resampling.bilinear,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "plt.figure(); \n",
    "lidar_3m.sel(band=1).plot.imshow(vmin=2900, vmax=3300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-wireless",
   "metadata": {},
   "source": [
    "### Extract 3DEP Snow-Off Elevations at ATL08 Snow-On Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a denser grid of the 3DEP data\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RectBivariateSpline.html\n",
    "\n",
    "interpolator = RectBivariateSpline(np.array(lidar_3m.y)[::-1], np.array(lidar_3m.x), \n",
    "                                   np.array(lidar_3m.sel(band=1))[::-1,:], kx=1, ky=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array(lidar_3m.x)\n",
    "y0=np.array(lidar_3m.y)\n",
    "\n",
    "\n",
    "ii = (march_11_clip['x_seg'] > np.min(x0)) & (march_11_clip['x_seg'] < np.max(x0))\n",
    "\n",
    "ii &= (march_11_clip['y_seg'] > np.min(y0)) & (march_11_clip['y_seg'] < np.max(y0))\n",
    "\n",
    "# Evaluate the spline at the ATL08 point locations, making an ndarray of the 3DEP elevations at the ATL08 point locations\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RectBivariateSpline.ev.html\n",
    "zi_1l = interpolator.ev(march_11_clip['y_seg'][ii], march_11_clip['x_seg'][ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of invalid values (in this case they are -9999)\n",
    "\n",
    "zi_1l[zi_1l <= 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for the datum offset between the datasets by subtracing 15.6 m from the 3DEP elevations\n",
    "\n",
    "zi_1l = zi_1l - 15.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-render",
   "metadata": {},
   "source": [
    "### Plot the location of the ICESat-2 track and the elevation of the snow-off and snow-on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[8, 5]); \n",
    "hax=fig.subplots(1,2)\n",
    "plt.sca(hax[0])\n",
    "lidar_3m.sel(band=1).plot.imshow(vmin=2900, vmax=3300)\n",
    "plt.plot(march_11_clip['x_seg'][ii], march_11_clip['y_seg'][ii],'.', c='orange')\n",
    "plt.axis('equal')\n",
    "plt.title('Grand Mesa ICESat-2 Track')\n",
    "\n",
    "plt.sca(hax[1])\n",
    "plt.plot(march_11_clip['y_seg'][ii], march_11_clip['h_te_best_fit'][ii],'.', label='March 11')\n",
    "plt.plot(march_11_clip['y_seg'][ii], zi_1l,'.', label='DTM')\n",
    "plt.legend()\n",
    "plt.title('ATL08 GT1L')\n",
    "plt.xlabel('x coordinate of projection [meter]')\n",
    "plt.ylabel('Elevation (m)')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('ATL08_gt1l.jpeg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-productivity",
   "metadata": {},
   "source": [
    "### Calculate Snow Depth by subtracting the snow off from the snow on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_1l = march_11_clip['h_te_best_fit'] - zi_1l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif1l = dif_1l.to_frame()\n",
    "dif1l.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first data point, which appears to be an outlier (perhaps too close to the edge of the mesa)\n",
    "\n",
    "dif_1l = dif_1l[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the basic statistics of the snow depth dataset\n",
    "\n",
    "dif_1l.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-linux",
   "metadata": {},
   "source": [
    "### Plot the Snow Depth at each point, and the mean snow depth along the ICESat-2 track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(dif_1l.index, dif_1l)\n",
    "ax.axhline(y=0, c = 'black', linestyle='-')\n",
    "ax.axhline(y=0.352486, c = 'grey', linestyle='--')\n",
    "ax.set_xlabel('Along Track Distance')\n",
    "ax.set_ylabel('Snow Depth (m)')\n",
    "ax.set_title('Snow Depth (m) Derived From ICESat-2 and 3DEP')\n",
    "#plt.savefig('SnowDepthScatter.jpeg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-organic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
